{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Regressão múltipla\n",
    "\n",
    "<br>\n",
    "\n",
    "### Índice <a name=\"topo\"></a>\n",
    "\n",
    "1. [Simulando a distribuição de $\\hat{\\beta}$](#1)\n",
    "\n",
    "\n",
    "2. [Testando hipóteses sobre os parâmetros](#2)\n",
    "\n",
    "\n",
    "3. [Variáveis Qualitativas](#3)\n",
    "\n",
    "\n",
    "4. [Qualidade do modelo](#4)\n",
    "    - $R^2$\n",
    "    - AIC\n",
    "    - $R^2_{ajustado}$\n",
    "\n",
    "5. [Seleção de variáveis](#5)\n",
    "    - forward\n",
    "    - backward\n",
    "    - stepwise  \n",
    "\n",
    "6. [Regularização](#6)\n",
    "    - L1 lasso\n",
    "    - L2 ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from scipy.stats import ks_2samp\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simulando a distribuição de $\\hat{\\beta}$</span><a name=\"1\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Vamos 50 observações X e y, com uma associação pré-determinada, seguindo a seguinte equação:\n",
    "\n",
    "$y = 5 + 0.1 x + \\epsilon$\n",
    "\n",
    "com o parâmetro aleatório de erro sendo: $\\epsilon \\thicksim N(0,0.5)$\n",
    "\n",
    "Usando a biblioteca ```random``` do numpy é bem fácil simular estes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y\n",
      "x  1.000000  0.480948\n",
      "y  0.480948  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 29 Oct 2021</td> <th>  Prob (F-statistic):</th> <td>0.000407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:27:40</td>     <th>  Log-Likelihood:    </th> <td> -31.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   67.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   71.22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    4.9235</td> <td>    0.130</td> <td>   37.953</td> <td> 0.000</td> <td>    4.663</td> <td>    5.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    0.1062</td> <td>    0.028</td> <td>    3.801</td> <td> 0.000</td> <td>    0.050</td> <td>    0.162</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.187</td> <th>  Durbin-Watson:     </th> <td>   2.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.911</td> <th>  Jarque-Bera (JB):  </th> <td>   0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.017</td> <th>  Prob(JB):          </th> <td>   0.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.004</td> <th>  Cond. No.          </th> <td>    9.47</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.231\n",
       "Model:                            OLS   Adj. R-squared:                  0.215\n",
       "Method:                 Least Squares   F-statistic:                     14.44\n",
       "Date:                Fri, 29 Oct 2021   Prob (F-statistic):           0.000407\n",
       "Time:                        16:27:40   Log-Likelihood:                -31.700\n",
       "No. Observations:                  50   AIC:                             67.40\n",
       "Df Residuals:                      48   BIC:                             71.22\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      4.9235      0.130     37.953      0.000       4.663       5.184\n",
       "x              0.1062      0.028      3.801      0.000       0.050       0.162\n",
       "==============================================================================\n",
       "Omnibus:                        0.187   Durbin-Watson:                   2.386\n",
       "Prob(Omnibus):                  0.911   Jarque-Bera (JB):                0.003\n",
       "Skew:                          -0.017   Prob(JB):                        0.999\n",
       "Kurtosis:                       3.004   Cond. No.                         9.47\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('figure', figsize=(6, 4))\n",
    "\n",
    "N = 50\n",
    "\n",
    "\n",
    "x = np.linspace(0,8,N)\n",
    "y = 5 + .1*x + np.random.randn(N)*.5\n",
    "\n",
    "df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "_ = sns.regplot(x='x', y='y', data = df1, ax = ax)\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(3, 8)\n",
    "ticks = ax.set_xticks(list(range(0,9,1)))\n",
    "ticks = ax.set_yticks(list(range(3,9,1)))\n",
    "\n",
    "print(df1.corr())\n",
    "\n",
    "reg = smf.ols('y ~ x', data = df1).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variações aleatórias\n",
    "\n",
    "Nessa situação, podemos considerar que estamos extraindo 50 observações das variáveis x e y de forma aleatória de uma população com as característias especificadas.\n",
    "\n",
    "Observe que a cada vez que rodamos a célula acima, obtemos um valor distinto de $\\beta$.\n",
    "\n",
    "Vamos fazer isso algumas vezes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "for i in range(2000):\n",
    "    x = np.linspace(0,8,N)\n",
    "    y = .1*x + np.random.randn(N)*.5\n",
    "    df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "    reg = smf.ols('y ~ x', data = df1).fit()\n",
    "    betas.append(reg.params[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure', figsize=(20, 10))\n",
    "g = sns.displot([betas], binwidth = .005, height = 5, aspect = 1.5)\n",
    "g.set(xlim=(0, 0.2), ylim=(0, 170))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os valores de $\\beta$ estão em torno do verdadeiro valor (0,1), embora sejam aleatórios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulando sob $H_0$\n",
    "A célula abaixo simula os dados com $\\beta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "for i in range(2000):\n",
    "    x = np.linspace(0,8,N)\n",
    "    y = 0*x + np.random.randn(N)*.5\n",
    "    df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "    reg = smf.ols('y ~ x', data = df1).fit()\n",
    "    betas.append(reg.params[1])\n",
    "    \n",
    "plt.rc('figure', figsize=(20, 10))\n",
    "g = sns.displot([betas], binwidth = .005, height = 5, aspect = 1.5)\n",
    "g.set(xlim=(-0.15, 0.15), ylim=(0, 170))\n",
    "\n",
    "# sns.displot(betas, binwidth = .005, height = 5, aspect = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os valores obtidos dos $\\beta$s se concentram em torno do 0,1, e a frequência diminui quanto mais nos afastamos do valor verdadeiro. Essa distribuição tem a \"cara\" de uma distribuição muito conhecida e presente em diversas situações, a distribuição Normal (ou Gaussiana). E sim, sob determinadas circunstâncias, a distribuição do $\\beta$ é de fato Normal e com um desvio padrão conhecido. O desvio padrão de um parâmetro em geral é chamado por outro nome **erro padrão**, e é ele que aparece na saída do statsmodels com o nome de ``` std err ```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testando hipóteses sobre os parâmetros</span><a name=\"2\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Queremos saber se uma variável é relevante. Em geral, transformamos as nossas hipóteses para uma afirmação falseável, e sob a qual conseguimos calcular probabilidades. Dessa forma, podemos formular a seguinte hipótese:\n",
    "\n",
    "$H_0: \\beta = 0$  \n",
    "$H_a: \\beta \\neq 0$\n",
    "\n",
    "Assim, sob $H_0$ temos que $\\hat{\\beta}$ dividido pela estimativa do seu erro padrão (*std err*) tem uma distribuição *t-Student* (que é bem parecida com a normal) centrada em zero. **Esse valor corresponde à coluna ```t```** na saída do statsmodels.    \n",
    "\n",
    "Em termos práticos significa que se $\\beta$ está muito longe do zero (comparado com o seu erro padrão), rejeitamos $H_0$, ou seja, se $H_0$ é falsa, significa que $H_a$ é verdadeira, ou seja, $\\beta$ é diferente de zero, o que significa que a variável é relevante no modelo de regressão. \n",
    "\n",
    "Caso contrário, não consideramos a variável relevante no modelo e ela pode ser retirada do modelo.\n",
    "\n",
    "Se você observou um valor $\\hat{\\beta}_{obs}$ como estimativa do seu beta, uma quantidade muito útil de se calcular é $p(|\\hat{\\beta}|>\\hat{\\beta}_{obs})$\n",
    "\n",
    "Um valor interessante na saída do statsmodels é o ```p>|t|```. Vamos lá: \n",
    "\n",
    "- **$\\hat{\\beta}$ é variável aleatória** ok? Primeiro entenda isso. Ele é uma função dos dados. Como os dados são variáveis aleatórias, qualquer função deles é variável aleatória também. Então ele é variável aleatória.\n",
    "- O **p-value** é a probabilidade de obtermos um $\\hat{\\beta}$ mais extremo (maior em valores absolutos) que o observado na nossa amostra, sob $H_0$.\n",
    "- **Regra prática**: então se o *p-value* é muito pequeno, digamos menor que $(1-\\gamma)$, **rejeitamos $H_0$** pois é muito pouco provável observar um beta como estes que observamos sob $H_0$ (Lembra... $H_0$ indica que $\\beta=0$ e a variável é irrelevante no modelo). Esse $\\gamma$ é o que chamamos de confiança e o $(1-\\gamma)$ de significância. Este é o famoso teste de significância aplicado à regressão.\n",
    "- **Regra de bolso**: muitas pessoas usam 5\\% como referência para o *p-value*, outras usam 1\\%. Esse assunto realmente dá pano pra manga e não vamos entrar na polêmica aqui. Mas em todo caso, lembre-se da frase do Box, de que \"todo modelo está errado\" inclusive o seu (e o meu também). A pergunta é o que torna ele útil? E o *p-value* é sem dúvida um valor muito útil.\n",
    "\n",
    "Vamos ver novamente isso no statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "\n",
    "\n",
    "x = np.linspace(0,8,N)\n",
    "y = 5 + .1*x + np.random.randn(N)*.5\n",
    "\n",
    "df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "_ = sns.regplot(x='x', y='y', data = df1)\n",
    "print(df1.corr())\n",
    "\n",
    "reg = smf.ols('y ~ x', data = df1).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos simular um caso em que $H_0$ é verdadeira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "for i in range(2000):\n",
    "    x = np.linspace(0,8,N)\n",
    "    y = 0*x + np.random.randn(N)*.5\n",
    "    df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "    reg = smf.ols('y ~ x', data = df1).fit()\n",
    "    betas.append(reg.params[1])\n",
    "    \n",
    "sns.displot(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observação sobre o teste de significância\n",
    "Repare que os valores simulados de $\\beta$ se distribuem ao longo do verdadeiro valor, que é o zero neste caso, e chegam bem próximoes de 0.1 e -0.1. Se fazemos o nosso teste com 5% de significância, quer dizer que 5% das vezes (1 em cada 20) $H_0$ vai ser verdadeira, mas vamos ter a conclusão errada. Esse é o famoso **erro tipo I** dos testes de hipóteses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variáveis qualitativas </span><a name=\"3\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Já vimos que para tratar variáveis qualitativas precisamos transformá-las em *dummies*, processo este conhecido como *\"hot encoding\"* ou simplesmente *\"encoding\"*.\n",
    "\n",
    "Antes de partir para o próximo tema, vamos mergulhar um pouco mais fundo no entendimento das variáveis *dummy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])\n",
    "tips['net_bill'] = tips['total_bill'] - tips['tip']\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(y = 'tip', x = 'size', data = tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = patsy.dmatrices('tip ~ C(size)', data = tips)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'size': [1, 2, 3, 5, 4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "dmatrix(\"C(size)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = patsy.dmatrices('tip ~ C(size)', data = tips)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sm.OLS(y, x).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.4375 + 1.1448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = patsy.dmatrices('tip ~ C(size, Treatment(2))', data = tips)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(y, x).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Qualidade do modelo e complexidade</span><a name=\"4\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Quando fazemos uma regressão múltipla, pelo próprio método de mínimos quadrados ordinários, a métrica $R^2$ vai ser necessariamente melhor sempre que adicionarmos uma variável a mais. Sempre. Por menos sentido que a variável faça, por menos informação que ela agregue, o $R^2$ vai ser maior (ou no pior extremo caso, igual) ao que tínhamos antes.\n",
    "\n",
    "Vamos ver isso na prática na base de gorjetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols('tip ~ C(size, Treatment(2)) + np.log(net_bill)', data = tips).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos inserir a variavel *day* e checar os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols('tip ~ C(size, Treatment(2)) + np.log(net_bill) + day', data = tips).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observações\n",
    "\n",
    "- O $R^2$ aumentou, embora a variável adicionada não pareça ser significante.\n",
    "- O $R^2$ sempre vai aumentar. Na pior das hipóteses ele fica igual.\n",
    "- O modelo ficou mais \"complicado\".\n",
    "- Estamos aumentando o risco de \"overfitting\".\n",
    "- Esta variável adicional interfere nas estimativas dos demais parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navalha de Occam\n",
    "\n",
    "Um princípio conhecido como *[Navalha de Occam](https://en.wikipedia.org/wiki/Occam%27s_razor)* indica que se temos dois modelos com indicadores iguais de qualidade, e um é mais simples, o mais simples é desejável. Dessa forma, diversas propostas surgem na tentativa de \"balisar\" a quantidade de parâmetros no modelo, como o $R^2$-*ajustado* - que sofre uma penalização por cada parâmetro no modelo e o AIC que vamos discutir adiante.\n",
    "\n",
    "Com isso em mente, há na literatura diversas alternativas para se considerar a complexidade do modelo na medida de qualidade, como o critério de Akaike (AIC) e o $R^2-ajustado$.\n",
    "\n",
    "#### AIC\n",
    "\n",
    "*Akaike´s Information Criterion* (ou critério da informação de Akaike). É uma métrica mais \"estatística\" de qualidade de ajuste do modelo, desenhada para comparar modelos com diferentes combinações de variáveis. Quanto menor o AIC, melhor o modelo - ou seja, se colocamos uma nova variável no modelo, por esse critério ela é relevante se o AIC diminuir, e não é relevante caso contrário. \n",
    "\n",
    "Diferente do $R^2$, o AIC depende do tamanho da amostra, de modo que não tem uma 'regra de bolso' do tipo \"perto de 1 é bom\", mas é adequado para comparar modelos na mesma amostra.\n",
    "\n",
    "A Wikipedia tem um artigo interessante sobre o [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion).\n",
    "\n",
    "#### $R^2-ajustado$\n",
    "\n",
    "O $R^2$-ajustado procura ponderar o incremento em explicação da variabilidade com o incremento em complexidade do modelo em termos de número de parâmetros. Ele aumenta se o $R^2$ aumentar mais do que o esperado \"por acaso\", e diminui caso contrário. Sua fórmula é a seguinte:\n",
    "\n",
    "$$R^2_{aj} = 1- \\left[ \\frac{(1-R^2)(n-1)}{(n-k-1)} \\right]$$\n",
    "\n",
    "#### Observações do exemplo anterior\n",
    "Repare que, no exercício anterior, quando inserimos uma variável irrelevante no modelo, o $R^2$ aumentou, mas o $R^2-ajustado$ diminuiu e o AIC aumentou, sugerindo que esta variável não deve entrar no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Seleção de modelos </span><a name=\"5\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Três algoritmos clássicos na literatura estatística para seleção de variáveis:\n",
    "\n",
    "- *Forward*:  \n",
    "    Parte de um modelo vazio e vai incluindo variáveis estatisticamente relevantes uma a uma, priorizando a mais relevante, até que nenhuma seja incluída. Pode haver alguma variável que deixou de ser relevante na presença daquelas que foram incluídas depois.\n",
    "    1. Definir um limite *LI* de *p-value* para uma variável ser **incluída** no modelo\n",
    "    2. Iniciar com um modelo sem variáveis\n",
    "    3. Para cada variável fora do modelo, testar $\\beta = 0$ na presença das demais - armazenar o *p-value*\n",
    "    4. Se o menor *p-value* for menor que *L*, a variável correspondente é incluída no modelo\n",
    "    5. Repetir 3 e 4 até que não sejam adicionadas variáveis ao modelo\n",
    "    <br><br>\n",
    "- *Backward*:  \n",
    "    Parte de um modelo com todas as variáveis possíveis consideradas e vai removendo-as uma a uma, até que nenhuma seja removida. Pode haver variáveis relevantes ainda após o término.\n",
    "    1. Definir um limite *LE* de *p-value* para uma variável ser **excluída** do modelo\n",
    "    2. Para cada variável incluída no modelo, testar $\\beta = 0$ na presença das demais - armazenar o *p-value*\n",
    "    3. Se o menor *p-value* for maior que *LE*, a variável é excluída do modelo\n",
    "    5. Repetir 3 e 4 até que não sejam excluídas mais variáveis do modelo\n",
    "- *Stepwise*:\n",
    "    É básicamente uma mistura dos dois. Vai incluindo variáveis, eventualmente removendo alguma variável caso seja irrelevante na presença das demais.\n",
    "    \n",
    "**Crítica**: Essa abordagem é criticada na comunidade porque esse *p-value* é tido mais como uma referência. Muitos usam esse algoritmo com o critério de Akaike ao invés do *p-value*, ou mesmo as regularizações L1 e L2, com a qual é possível fazer um *grid search* para buscar melhores resultados em previsão.\n",
    "\n",
    "De qualquer forma, a seleção de um modelo por um desses algoritmos **muito raramente** (pra não dizer nunca) é a escolha final. Sempre há insights e ajustes a serem feitos, categorias a agrupar, variáveis conceitualmente importantes que podem ser priorizadas, multicolinearidade a ser tratada (mais sobre isso adiante), enfim, é um processo meio arte meio ciência suportado por algoritmos menos que executado por algoritmos.\n",
    "\n",
    "O código abaixo foi extraído e adaptado do fórum [*stackovervlow*](https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm), da resposta do David Dale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.05, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype=np.dtype('float64'))\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.index[new_pval.argmin()]\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        print(\"#############\")\n",
    "        print(included)\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "variaveis = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(variaveis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularização </span><a name=\"6\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Regularização (ou *model regularization*) é uma forma de considerar a complexidade adicionada ao modelo e simplificar o modelo, quer seja por deixar os parâmetros menos relevantes, quer seja por retirá-los intrgralmente do modelo.\n",
    "\n",
    "As duas formas mais populares na literatura do aprendizado de máquina são a regularização L1 e a regularização L2:\n",
    "\n",
    "#### Função de perda\n",
    "Vamos relembrar que a nossa regressão é uma regressãod e mínimos quadrados, ou seja, estamos minimizando a função do erro quadrático médio (EQM) em função dos parâmetros do modelo ($\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$). Nossa função de erro, podemos chamá-la de um nome mais geral: *função de perda* L:\n",
    "\n",
    "$$L = \\sum_{n=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2$$\n",
    "\n",
    "As formas de regularização mais populares introduzem uma \"penalização\" na função de perda devido ao aumento na complexidade do modelo - isto é, devido ao aumento do número de parâmetros (ou variáveis) no modelo.\n",
    "\n",
    "#### Regularização L1 (lasso)\n",
    "A regressão lasso introduz uma penalidade igual ao quadrado da soma dos coeficientes na função de perda:\n",
    "\n",
    "$$L_1 = \\sum_{i=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2 + \\alpha \\sum_{k=0}^{M} \\left| \\beta_k \\right|$$\n",
    "\n",
    "Em que:  \n",
    "- $\\beta_k$ são os parâmetros do modelo (atenção que $\\beta_0$ é o intercepto).\n",
    "- N é o número de observações\n",
    "- M é o número de parâmetros\n",
    "- $\\alpha$ no statsmodels é um *hiperparâmetro* do modelo, que regula a penalidade por complexidade.\n",
    "\n",
    "Dessa forma, minimizando essa função de perda, os parâmetros do modelo tendem a ter valor absoluto menor, e caso tragam mais complexidade que explicação da variância, são \"zerados\", o que significa que a variável correspondente fica é eliminada do modelo.\n",
    "\n",
    "#### Regularização L2 (ridge)\n",
    "Outra forma de regularização é a chamada regularização *ridge*, que minimiza a seguinte perda:\n",
    "\n",
    "$$L_2 = \\sum_{i=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2 + \\alpha \\sum_{k=0}^{M} \\left| \\beta_k \\right|^2$$\n",
    "\n",
    "Em que:  \n",
    "- $\\beta_k$ são os parâmetros do modelo (atenção que $\\beta_0$ é o intercepto).\n",
    "- N é o número de observações\n",
    "- M é o número de parâmetros\n",
    "- $\\alpha$ no statsmodels é um *hiperparâmetro* do modelo, que regula a penalidade por complexidade.\n",
    "\n",
    "Essa regularização é semelhante ao *lasso*, porém a penalização é no valor absoluto dos parâmetros. Diferente do lasso, não costuma \"zerar\" os parâmetros das variáveis menos relevantes, somente reduzir os coeficientes.\n",
    "\n",
    "#### *Elastic net*\n",
    "Uma forma bem popular de regularização de regressão é o *elastic net*, que consiste na mistura dos dois otimizando a seguinte função de perda:\n",
    "\n",
    "$$L_E = \\sum_{i=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2 \n",
    "    + \\alpha \\left( L1_{wt} \\sum_{k=0}^{M} \\left| \\beta_k \\right|\n",
    "                    + (1-L1_{wt}) \\sum_{k=0}^{M} \\left( \\beta_k \\right)^2\n",
    "             \\right)$$\n",
    "\n",
    "com:  \n",
    "- N é o número de observações e M o número de parâmetros\n",
    "- $\\alpha$ sendo o hiperparâmetro que dá importância à penalização  \n",
    "- $L1_{wt}$ sendo um número entre 0 e 1 \n",
    "    - quando vale 1, equivale regulaziração L1 - lasso\n",
    "    - quando 0 equivale a L2 - ridge\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos testar\n",
    "\n",
    "Vamos usar o Lasso, pois é uma forma interessante de fazer seleção de variáveis no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization\n",
    "modelo = 'tip ~ C(size) + np.log(net_bill) + smoker + time + day'\n",
    "md = smf.ols(modelo, data = tips)\n",
    "reg = md.fit_regularized(method = 'elastic_net' \n",
    "                         , refit = True\n",
    "                         , L1_wt = 1\n",
    "                         , alpha = 0.01)\n",
    "\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  LSTAT                          with p-value 5.0811e-88\n",
      "#############\n",
      "['LSTAT']\n",
      "Add  RM                             with p-value 3.47226e-27\n",
      "#############\n",
      "['LSTAT', 'RM']\n",
      "Add  PTRATIO                        with p-value 1.64466e-14\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO']\n",
      "Add  DIS                            with p-value 1.66847e-05\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS']\n",
      "Add  NOX                            with p-value 5.48815e-08\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX']\n",
      "Add  CHAS                           with p-value 0.000265473\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS']\n",
      "Add  B                              with p-value 0.000771946\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B']\n",
      "Add  ZN                             with p-value 0.00465162\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN']\n",
      "Add  CRIM                           with p-value 0.0445675\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM']\n",
      "Add  RAD                            with p-value 0.00169218\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD']\n",
      "Add  TAX                            with p-value 0.000521424\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD', 'TAX']\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD', 'TAX']\n",
      "resulting features:\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD', 'TAX']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.05, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype=np.dtype('float64'))\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.index[new_pval.argmin()]\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        print(\"#############\")\n",
    "        print(included)\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "variaveis = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(variaveis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stepwise = sm.OLS(y, sm.add_constant(pd.DataFrame(X[variaveis]))).fit()\n",
    "reg_stepwise.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Best subsets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = getBest(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models.sort_values('RSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mu = 0\n",
    "\n",
    "sigma = 1\n",
    "x = np.linspace(-5, 5, 500)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "\n",
    "xinf = np.linspace(-5, stats.norm.ppf(.025, 0, 1), 500)\n",
    "plt.fill_between(xinf, stats.norm.pdf(xinf, 0, 1), color = 'orange')\n",
    "\n",
    "xsup = np.linspace(stats.norm.ppf(.975, 0, 1), 5 , 500)\n",
    "plt.fill_between(xsup, stats.norm.pdf(xsup, 0, 1), color = 'orange')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinf = np.linspace(-5, stats.norm.ppf(.025, 0, 1), 500)\n",
    "plt.fill_between(xinf, stats.norm.pdf(xinf, 0, 1))\n",
    "\n",
    "xsup = np.linspace(stats.norm.ppf(.975, 0, 1), 5 , 500)\n",
    "plt.fill_between(xsup, stats.norm.pdf(xsup, 0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "from patsy import balanced\n",
    "from patsy import dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patsy.dmatrix(\"C(a, Diff)\", balanced(a=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced(a=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from patsy import demo_data\n",
    "\n",
    "data = demo_data(\"a\", nlevels=3)\n",
    "l = [\"a3\", \"a2\", \"a1\"]\n",
    "\n",
    "dmatrix(\"C(a, levels=l)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = [[0, 0], [0, 1], [1, 1]]\n",
    "\n",
    "dmatrix(\"C(a, ordinal)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
